{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3499,"status":"ok","timestamp":1674638018526,"user":{"displayName":"Zuzanna Skorniewska","userId":"12442564233068447969"},"user_tz":0},"id":"5iUurpTfi05Q"},"outputs":[],"source":["import torch\n","from torch import Tensor,nn\n","import torch.nn.functional as f\n","import sys \n","import numpy as np \n","from google.colab import files,drive\n","from gensim.models import Word2Vec"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24128,"status":"ok","timestamp":1674638042615,"user":{"displayName":"Zuzanna Skorniewska","userId":"12442564233068447969"},"user_tz":0},"id":"2WbFejTzffg5","outputId":"60646cb6-5c23-467a-ba46-fb9e1b145d00"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":53,"status":"ok","timestamp":1674638042618,"user":{"displayName":"Zuzanna Skorniewska","userId":"12442564233068447969"},"user_tz":0},"id":"y5Oq2twqjsrU"},"outputs":[],"source":["def attention_mechanism(query: Tensor, key: Tensor,value: Tensor,mask: Tensor) -> Tensor:\n","  num=query.bmm(key.transpose(1,2))\n","  num_masked = num.masked_fill(mask == 0, float(\"-1e20\"))\n","  scale = query.size(-1) ** 0.5\n","  softmax = f.softmax(num_masked/scale,dim=-1)\n","  out = softmax.bmm(value)\n","  return out \n","\n","def position_encoding(seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\") )-> Tensor:\n","    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n","    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n","    phase = pos / (1e4 ** (dim / dim_model))\n","\n","    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))\n","\n","def feed_forward(dim_input: int= 512, dim_feedforward : int = 2048) -> nn.Module:\n","  return nn.Sequential(\n","      nn.Linear(dim_input,dim_feedforward),\n","       nn.ReLU(),       \n","       nn.Linear(dim_feedforward, dim_input))\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":55,"status":"ok","timestamp":1674638042629,"user":{"displayName":"Zuzanna Skorniewska","userId":"12442564233068447969"},"user_tz":0},"id":"fIQiuVlMkWr0"},"outputs":[],"source":["class AttentionHead(nn.Module):\n","  def __init__(self,dim_in: int,dim_q:int,dim_k :int):\n","    super().__init__()\n","    self.q = nn.Linear(dim_in,dim_q)\n","    self.k = nn.Linear(dim_in,dim_k)\n","    self.v = nn.Linear(dim_in,dim_k)\n","\n","  def forward(self,query: Tensor,key: Tensor,value : Tensor, mask: Tensor) -> Tensor:\n","    return attention_mechanism(self.q(query),self.k(key),self.v(value),mask)\n","\n","\n","class MultiHeadAttention(nn.Module):\n","  def __init__(self,num_heads : int, dim_in: int, dim_q : int, dim_k : int):\n","    super().__init__()\n","    self.heads = nn.ModuleList(\n","        [AttentionHead(dim_in,dim_q,dim_k) for _ in range(num_heads)]\n","    )\n","    self.linear = nn.Linear(num_heads * dim_k,dim_in)\n","\n","  def forward(self,query: Tensor,key: Tensor, value: Tensor,mask: Tensor) -> Tensor:\n","    out_attention = [h(query,key,value,mask) for h in self.heads]\n","    out_cat = torch.cat(out_attention,dim=-1)\n","    out_lin = self.linear(out_cat)\n","    return out_lin\n","\n","\n","class Residual(nn.Module):\n","  def __init__(self,sublayer: nn.Module, dimension: int,dropout: float = 0.1):\n","    super().__init__()\n","    self.sublayer = sublayer\n","    self.norm = nn.LayerNorm(dimension)\n","    self.dropout = nn.Dropout(dropout)\n","  \n","  def forward(self,*tensors:Tensor) -> Tensor:\n","    return self.norm(tensors[0]+self.dropout(self.sublayer(*tensors)))\n","\n"]},{"cell_type":"code","execution_count":242,"metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1674644169481,"user":{"displayName":"Zuzanna Skorniewska","userId":"12442564233068447969"},"user_tz":0},"id":"XlUFIDmwoPii"},"outputs":[],"source":["from torch.nn.modules.activation import MultiheadAttention\n","\n","class Embedding(nn.Module):\n","  \n","  def __init__(self,embed_path:str=\"drive/MyDrive/chess/chess_embedding_sep_model/chess2vec.model\"):\n","    super().__init__()\n","    self.embed_layer=Word2Vec.load(embed_path)\n","    self.dim_embed  = self.embed_layer.vector_size\n","    self.corpus_length = len(self.embed_layer.wv.vocab)\n","    self.index_to_word = {key:value for (key,value) in enumerate(self.embed_layer.wv.vocab.keys())}\n","    self.word_to_index = {value:key for (key,value) in enumerate(self.embed_layer.wv.vocab.keys())}\n","\n","    \n","  def embed(self,src):\n","    return torch.Tensor(np.array([self.embed_layer.wv[key] for key in src]))\n","\n","  def translate_itw(self,src):    \n","    return np.vectorize(self.index_to_word.__getitem__)(src)\n","\n","\n","class TransformerDecoderLayer(nn.Module):\n","\n","  def __init__(self,\n","               dim_model:int = 512,\n","               num_heads:int = 6,\n","               dim_feedforward:int = 2048,\n","               dropout:float=0.1):\n","    \n","    super().__init__()\n","    dim_q = dim_k = max(dim_model // num_heads, 1)\n","    self.attention = Residual(\n","        MultiHeadAttention(num_heads,dim_model,dim_q,dim_k),\n","        dimension = dim_model,\n","        dropout = dropout)\n","    self.feed_forward = Residual(\n","        feed_forward(dim_model,dim_feedforward),\n","        dimension = dim_model,\n","        dropout = dropout)\n","    \n","  def forward(self,src: Tensor,mask:Tensor) -> Tensor:\n","    src = self.attention(src, src, src,mask)\n","    return self.feed_forward(src)\n","\n","\n","\n","class Decoder(nn.Module):\n","  def __init__(self,\n","              embed_path:str=\"drive/MyDrive/chess/chess_embedding/chess2vec.model\",\n","              num_layers:int=6,\n","              num_heads:int=6,\n","              dim_feedforward:int=2048,\n","              dropout:float=0.1):\n","    super().__init__()\n","    self.embed_layer=Embedding(embed_path=embed_path)\n","    self.layers = nn.ModuleList([TransformerDecoderLayer(self.embed_layer.dim_embed,num_heads,dim_feedforward,dropout) for _ in range(num_layers)])\n","    self.linear = nn.Linear(self.embed_layer.dim_embed,self.embed_layer.corpus_length)\n","\n","\n","  def masking(self,batch_size,seq_len):\n","        \"\"\"\n","        Args:\n","            trg: target sequence\n","        Returns:\n","            trg_mask: target mask\n","        \"\"\"\n","        # returns the lower triangular part of matrix filled with ones\n","        mask = torch.tril(torch.ones((seq_len,seq_len))).expand(\n","            batch_size,seq_len,seq_len)\n","        \n","        return mask\n","    \n","\n","  def forward(self, src: Tensor) -> Tensor:\n","    src = self.embed_layer.embed(src)\n","    batch_size,seq_len, dimension = src.size(0),src.size(1), src.size(2)\n","    src += position_encoding(seq_len, dimension)\n","    mask = self.masking(batch_size,seq_len)\n","    for layer in self.layers:\n","        src = layer(src,mask)\n","    out = self.linear(src)\n","    return torch.softmax(out,dim=-1)\n","\n","\n","class ChessTransformer(nn.Module):\n","  def __init__(self,\n","              embed_path:str=\"drive/MyDrive/chess/chess_embedding/chess2vec.model\",\n","               num_layers:int=6,\n","               num_heads:int=6,\n","               dim_feedforward:int=2048,\n","               dropout:float=0.1):\n","    super().__init__()\n","    self.decoder = Decoder(embed_path=embed_path,\n","                           num_layers=num_layers,\n","                           num_heads=num_heads,\n","                           dim_feedforward=dim_feedforward,\n","                           dropout=dropout)\n","    \n","    self.embedding = Embedding(embed_path=embed_path)\n","\n","\n","  def forward(self, src: np.array) -> Tensor:\n","    out = self.decoder(src)\n","    return out\n","\n","\n","  def decode(self,src,num_moves):\n","      \"\"\"\n","      for inference\n","      Args:\n","          src: input to decoder\n","      out:\n","          out_labels : returns final prediction of sequence\n","      \"\"\"\n","\n","\n","      src = np.array(src)\n","      if len(src.shape)==1:\n","          src=np.expand_dims(src,0)\n","      out_seq = src\n","      for i in range(num_moves):\n","        out = self.decoder(out_seq) #bs x seq_len x vocab_d\n","        out = out[:,-1,:] # taking the last token\n","        out = torch.unsqueeze(out,axis=1)\n","        out = torch.argmax(out,-1)\n","        out = self.embedding.translate_itw(out)\n","        out_seq = np.append(out_seq,out,axis=1)\n","\n","      return out_seq\n","\n"]},{"cell_type":"code","execution_count":236,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":265,"status":"ok","timestamp":1674643965581,"user":{"displayName":"Zuzanna Skorniewska","userId":"12442564233068447969"},"user_tz":0},"id":"lf5j6pDTjzks","outputId":"653a1b51-0523-4964-92c3-bade46edea3b"},"outputs":[{"data":{"text/plain":["(2, 10)"]},"execution_count":236,"metadata":{},"output_type":"execute_result"}],"source":["data = [['Nb1-a3',\n"," 'f7-f5',\n"," 'f2-f4',\n"," 'Ng8-f6',\n"," 'Ng1-h3',\n"," 'd7-d5',\n"," 'Na3-b5',\n"," 'Bc8-d7',\n"," 'Ke1-f2',\n"," 'Nb8-c6'],['Nb1-a3',\n"," 'f7-f5',\n"," 'f2-f4',\n"," 'Ng8-f6',\n"," 'Ng1-h3',\n"," 'd7-d5',\n"," 'Na3-b5',\n"," 'Bc8-d7',\n"," 'Ke1-f2',\n"," 'Nb8-c6']]\n"," \n","data = np.array(data)\n","data.shape"]},{"cell_type":"code","execution_count":237,"metadata":{"executionInfo":{"elapsed":833,"status":"ok","timestamp":1674643967552,"user":{"displayName":"Zuzanna Skorniewska","userId":"12442564233068447969"},"user_tz":0},"id":"yY9cMyZOeFFV"},"outputs":[],"source":["mdl = ChessTransformer()"]},{"cell_type":"code","execution_count":238,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1674643968847,"user":{"displayName":"Zuzanna Skorniewska","userId":"12442564233068447969"},"user_tz":0},"id":"CpGEiUFGnJhH","outputId":"dd935ab2-d56f-465b-dbc7-1c8caf6ebf3a"},"outputs":[{"data":{"text/plain":["torch.Size([2, 10, 21571])"]},"execution_count":238,"metadata":{},"output_type":"execute_result"}],"source":["src = torch.rand(5, 10, 500) # batch size, seq length, embedding dimension\n","\n","out = mdl.forward(data)\n","out.shape"]},{"cell_type":"code","execution_count":239,"metadata":{"executionInfo":{"elapsed":413,"status":"ok","timestamp":1674643970360,"user":{"displayName":"Zuzanna Skorniewska","userId":"12442564233068447969"},"user_tz":0},"id":"DdP4Z05y1Fxk"},"outputs":[],"source":["out=mdl.decode(['e2-e4','e2-e3'],5)"]},{"cell_type":"code","execution_count":241,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":320,"status":"ok","timestamp":1674643975537,"user":{"displayName":"Zuzanna Skorniewska","userId":"12442564233068447969"},"user_tz":0},"id":"oVyklLE0ThMz","outputId":"c12b4bdc-ead8-4744-c4b6-b84583449be7"},"outputs":[{"data":{"text/plain":["array([['e2-e4', 'e2-e3', 'Nc6-a5#', 'Bd7-c6+', 'Rb4xb6+', 'Qa3xa2#',\n","        'Bf8-e7#']], dtype='<U7')"]},"execution_count":241,"metadata":{},"output_type":"execute_result"}],"source":["out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhk-pJN_TrQn"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPsDX06S1kUCAtSF6+3N0ak","provenance":[]},"kernelspec":{"display_name":"Python 3.9.0 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.0"},"vscode":{"interpreter":{"hash":"e1270bf742471f08454fdc592834658bbc8e48a81afcac5abe53248a989e7303"}}},"nbformat":4,"nbformat_minor":0}
